---
title: "Experiment_segments_waveform_pendigit_letter"
author: "Tian Qin"
date: '2023-11-04'
output: html_document
---



### SBPMT complie

```{r}

###complie rcpp file for probitboost first

Rcpp::sourceCpp("C:/Users/63422/Desktop/SBPMT/src/rcpp_wpbt.cpp")


### complie working functions for SBPMT
PMT <- function(xtrain,ytrain,w,depth=5,min_size=20,M=10){
  
  base_tree <- rpart(factor(ytrain)~.,data=xtrain,weights=w,control=list(maxdepth=depth,minsplit =min_size))
  
  terminal_nodes <- sort(unique(base_tree$where)) #pates0('n',sort(unique(base_tree$where))
  
  pbts <- list()#vector("list",length = length(terminal_nodes))
  
  for(node in terminal_nodes){
    index_node <- as.numeric(which(base_tree$where == node))
    #print(index_node)
    
    x_node <- xtrain[index_node,]
    y_node <- ytrain[index_node]
    names(y_node)<- row.names(x_node)
    w_node <- w[index_node]
    if(length(unique(y_node))==1){
      
      pbts[[paste0("n",node)]] <- list(predicted=unique(y_node),unique=TRUE,fitted=y_node)
    }else{
      
      fit <- WoProbitBoost(x_node, y_node, Wo=w_node, M_max = M)
      names(fit$fitted_p)<- row.names(x_node)
      pbts[[paste0("n",node)]] <-list(boost_feats=fit$boost_feat,local_nodes=index_node,fitted=fit$fitted_p, ynode=y_node,unique=FALSE)#MWProbitBoost(x_node, y_node, Wo=w_node,M_max=M,aic=aic)
    }
    
    
    #LogitBoost(x_node, y_node, nIter = ncol(x_node)) #WoProbitBoost(x_node, y_node, Wo=w_node,M_max=M,aic=aic)$boost_feat #PB(x_node,y_node,M=M,aic=aic,depth = depth,min_size = min_size)
    
  }
  #object <- list(pbt=pbts,terminal_nodes=terminal_nodes)
  
  return(list(tree=base_tree,pbt=pbts,terminal_nodes=terminal_nodes))
}




pmt.predict<- function(pmt_list,new_data){
  
  tree <- pmt_list$tree
  ter_nodes <- pmt_list$terminal_nodes
  
  #pbts <- pmt_list$pbt
  
  pred_nodes_index <- rpart:::pred.rpart(tree, rpart:::rpart.matrix(new_data))
  pred_nodes <- paste0("n",as.numeric(pred_nodes_index))
  
  predicted <- predictProbit(as.matrix(new_data),pmt_list,pred_nodes) #numeric(length = length(pred_nodes))
  
  
  return(predicted)
}


AdaPMT <- function(ms=10,M=5,depth=5,trainx,trainy,xtest,step=0.5,w_init,tpx_init,size=15,lab_list){
  n <- length(trainy)
  
  tpx <- tpx_init
  cpx <- tpx
  w <- w_init
  px <- numeric(n)
  re <- 0
  n_Class <- length(unique(trainy))
  Cx <- matrix(0,ncol=n_Class,nrow(xtest))
  lab_list <- lab_list#as.vector(sort(unique(trainy)))
  if(n_Class==2){
    for(m in 1:ms){
      # print(w)

      pmt <- PMT(trainx,trainy,w=w,depth=depth,M=M,min_size=size)
      terminode <- paste0("n",pmt$terminal_nodes)
      
      pred <- ifelse(do.call("c",lapply(terminode,function(node) pmt$pbt[[node]]$fitted ))>0.5,1,0)

      #print(Ix)
      #print(pred[names(trainy)])
      # pred <- ifelse(pmt.predict2(pmt,trainx)>0.5,1,0)
      Ix <- ifelse(pred[names(trainy)]!=trainy,1,0)
      err <- sum(w*Ix)/(sum(w))
      
      
      alpha <-step*log((1-err+1e-24)/(err+1e-24))
      
      w <- w*exp(alpha*Ix)
      w <- w/sum(w)
      # print(pmt$pbt)
      
      
      test <- pmt.predict(pmt,xtest)
      # print(test)
      
      test <- ifelse(test>0.5,1,-1)  
      
      tpx <-  tpx +test*alpha
      
      #tpx <-  tpx +test*alpha
      #  print(m)
      
    } 
    return(tpx)
  }else{
    
    for(m in 1:ms){
      boost_feat_train <- matrix(-Inf,ncol=n_Class,nrow=n)   
      boost_feat_test<- matrix(-Inf,ncol=n_Class,nrow=nrow(xtest))   # ... recursivly
      for (jClass in 1:n_Class) {
        #print('loop')
        y = as.numeric(trainy==lab_list[jClass]) # lablist[jClass]->1; rest->0
        pmt <- PMT(trainx,y,w=w,depth=depth,M=M,min_size=size)
        #pred <- pmt.predict(pmt,trainx,K2)
        
        terminode <- paste0("n",pmt$terminal_nodes)
        pj <- do.call("c",lapply(terminode,function(node) pmt$pbt[[node]]$fitted ))
        boost_feat_train[,jClass] <- pj[names(trainy)]# ifelse(do.call("c",lapply(terminode,function(node) pmt$pbt[[node]]$fitted ))>0.5,1,0)#pmt.predict2(pmt,trainx)#pmt$predict(trainx)
        
        boost_feat_test[,jClass] <- pmt.predict(pmt,xtest)
      }
      pred <- apply(boost_feat_train,1,function(row) lab_list[which.max(row)] )
      # print(boost_feat_train)
      #print(pred)
      #  pred <- ifelse(pmt$predict(trainx)>0,lablist[2],lablist[1])
      Ix <- ifelse(pred!=trainy,1,0)
      
      err <- sum(w*Ix)/(sum(w))
      
      
      alpha <-step*log((1-err+1e-24)/(err+1e-24))+log(n_Class-1)
      
      w <- w*exp(alpha*Ix)
      w <- w/sum(w)
      
      test <- apply(boost_feat_test,1,function(row) which.max(row) )#ifelse(pmt$predict(xtest)==1,1,-1)  
      
      
      for(row in 1:nrow(xtest)){
        
        Cx[row,test[row]] <- Cx[row,test[row]] +alpha
      }
      
      #print(m)
      
      
    }
    
    
    return(Cx)
    
    
    
  }
  
}



SBPMT <- function(n_tree=20,n_iteration=5,M=5,depth=5,xtrain,ytrain,xtest,step=0.5,size=15,alpha=0.7,seed=NULL){
  
  # n_tree <- n_tree
  # n_iteration <- n_iteration
  # 
  if(is.null(xtest)){
    xtest=xtrain
  }
  lab_list = as.vector(sort(unique(ytrain)))
  if(length(unique(ytrain))==2){
    
    rf_test <- matrix(0,ncol=n_tree,nrow=nrow(xtest))#$numeric(nrow(xtest))
    hist_accs <- numeric(n_tree)
    if(!is.null(seed)){
      set.seed(42)
    }

    for(n in 1:n_tree){
      
      samp_index <-  sample(1:nrow(xtrain),size=ceiling(alpha*nrow(xtrain)),replace = FALSE) #sample(1:nrow(xtrain),replace = TRUE)
      samp_fea <- 1:ncol(xtrain)#sample(1:ncol(xtrain),size=floor(sqrt(ncol(xtrain))),replace = FALSE)
      usp <- unique(samp_index)
      #print(length(usp))
      x_rf <- xtrain[samp_index,]
      y_rf <- ytrain[samp_index]
      
      w_init <- rep(1/length(usp),length(usp))
      
      tpx_init <- numeric(nrow(xtest))
      
      
      tpx_rf <- AdaPMT(ms=n_iteration,M=M,depth=depth,trainx=x_rf,trainy=y_rf,xtest=xtest,step=step,tpx_init=tpx_init,w_init=w_init,size=size,lab_list=lab_list)
      rf_test[,n] <- ifelse(tpx_rf>0,1,-1)
      
      
    }
    pred_binary <- apply(rf_test,1,function(row) ifelse(sum(row)>0,lab_list[2],lab_list[1]))
    return(pred_binary)
    
  }else if(length(unique(ytrain))>2){#multi-class, one-vs-all strategy used
    
    
    rf_test <- array(0,dim=c(n_tree,nrow(xtest),length(unique(ytrain))))#matrix(0,ncol=n_tree,nrow=nrow(xtest))#$numeric(nrow(xtest))
    hist_accs <- numeric(n_tree)
    
    
    for(n in 1:n_tree){
      
      samp_index <- sample(1:nrow(xtrain),size=alpha*nrow(xtrain),replace = FALSE)#sample(1:nrow(xtrain),replace = TRUE)
      samp_fea <-   sample(1:ncol(xtrain),size=floor(sqrt(ncol(xtrain))),replace = FALSE)#1:ncol(xtrain) #unique(sample(1:ncol(xtrain),replace = TRUE))#unique(sample(1:ncol(xtrain),replace = TRUE)
      usp <- unique(samp_index)
      #print(length(usp))
      x_rf <- xtrain[samp_index,]
      y_rf <- ytrain[samp_index]
 
      w_init <- rep(1/length(samp_index),length(samp_index))
      
      tpx_init <- numeric(nrow(xtest))
      
      tpx_rf <- AdaPMT(ms=n_iteration,M=M,depth=depth,trainx=x_rf,trainy=y_rf,xtest=xtest,step=step,tpx_init=tpx_init,w_init=w_init,size=size,lab_list=lab_list)

      rf_test[n,,] <-tpx_rf 

      
    }
    
    
    weights <- hist_accs/sum(hist_accs)
    
    pred_multi<- apply(apply(rf_test,c(1,2),function(block) which.max(block)  ), 2,function(col) lab_list[as.numeric(names(which.max(table(col))))])
    
    return(pred_multi)
    
  }

}








```


#Segments

```{r}
library(randomForest)
library(dplyr)
library(gbm)
#library(SBPMT) 
 library(caret)
library(rpart)
library(RWeka)
library(adabag)
library(xgboost)
SegmentsPART1<- read.table("segmentation.data", skip=1,sep=",",header=FALSE)
SegmentsPART2<- read.table("segmentationtest.test", skip=1,sep=",",header=FALSE)

Segments <- rbind(SegmentsPART1,SegmentsPART2)


set.seed(42)
df<-Segments[sample(nrow(Segments)),]
 df$V1   <- ifelse(df$V1  =='BRICKFACE',1,df$V1 )
 df$V1   <- ifelse(df$V1  =='CEMENT',2,df$V1 )
 df$V1   <- ifelse(df$V1  =='FOLIAGE',3,df$V1 )
 df$V1   <- ifelse(df$V1  =='GRASS',4,df$V1 )
 df$V1   <- ifelse(df$V1  =='PATH',5,df$V1 )
 df$V1   <- ifelse(df$V1  =='SKY',6,df$V1 )
  df$V1   <- ifelse(df$V1  =='WINDOW',7,df$V1 )
#   dmy<- dummyVars(" ~V2 + V3+ V7+V8", data = df)
#  cat_df <-  data.frame(predict(dmy, newdata = df))
#  num_df <- df[,c('V1','V4','V5','V6','V9','V10')]
# # 
# processed_df <- as.data.frame(cbind(cat_df,num_df))


#Create 10 equally size folds
folds <- caret::createFolds(factor(df$V1), k = 10,list = FALSE)#cut(seq(1,nrow(df)),breaks=10,labels=FALSE)

cvpbt <- c()
cvbpbtrf <- c() 
cvdt <- c()
cvgbm <- c()
cvrf <- c()
clmt <- c()
cvada <- c()
cvxg <- c()
cvxg10 <- c()
for(i in 1:10){
    #Segement your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    xtest<- df[testIndexes, 2:20]
    #xtest<- xtest[,-2]
    ytest <- df[testIndexes,1]
    xtrain <- df[-testIndexes, 2:20]
    #xtrain <- xtrain[,-2]
    ytrain <- df[-testIndexes,1] 
    names(ytrain) <- row.names(xtrain)
    
        ###SBPMT
    
    sbpmt_pred <- SBPMT(n_tree=21,n_iteration=5,M=100,depth=6,xtrain=xtrain,ytrain=ytrain,xtest=xtest,step=0.5,size=20,alpha=0.7)
    rft <-table(sbpmt_pred,ytest)
    cvbpbtrf[i] <- sum(diag(rft))/sum(rft)
    print(cvbpbtrf[i])


     ###GradientBoost
    lab_list = as.vector(sort(unique(ytrain)))
n_Class <- length(unique(ytrain))
boost_feat_test<- matrix(0,ncol=n_Class,nrow=nrow(xtest))  
        for (jClass in 1:n_Class) {
          #print('loop')
          yt = as.numeric(ytrain==lab_list[jClass]) # lablist[jClass]->1; rest->0
        gbm.model = gbm(yt~., data=xtrain, distribution = 'bernoulli',bag.fraction=0.7)

          boost_feat_test[,jClass] <- predict.gbm(gbm.model, xtest,type='response')
        }
        pred_y <- apply(boost_feat_test,1,function(row) lab_list[which.max(row)] )


    conf_mat= confusionMatrix(as.factor(pred_y),as.factor(ytest))
    acct_gbm <- (sum(diag(conf_mat$table)))/( sum(conf_mat$table))
    cvgbm[i] <-acct_gbm
    print(cvgbm[i])
    ### RandomForest 

     ytrain <- factor(ytrain)
    tree <- randomForest(ytrain~., data = xtrain)
    p <-predict(tree, xtest)  #ifelse(predict(tree, xtest)>0.5,1,0) #predict(tree, xtest,type = 'class') #ifelse(predict(tree, xtest,type = 'class')>0.5,1,0)
    confmatrix_treer <-table(p, ytest)
    acctr <- (sum(diag(confmatrix_treer)))/( sum(confmatrix_treer))
    cvrf[i] <-acctr
    print(cvrf[i])

    
    ###Adaboost
         data_adb <- df [-testIndexes,]
     data_adb$V1<- as.factor(data_adb$V1)
    #dt<- data.frame(ytrain=ytrain,xtrain=xtrain)
    ada.model = boosting(V1~., data = data_adb)
    predada = predict.boosting(ada.model , newdata=df [testIndexes,])
    cvada[i] <- sum(diag(predada$confusion))/sum(predada$confusion)
    print(cvada[i])
    
    ###Xgboost(100)
            train_data   <- as.matrix(df [-testIndexes,2:20])
train_label  <- as.numeric(as.factor(df [-testIndexes,1]))-1
train_matrix <- xgb.DMatrix(data = train_data, label = train_label)
# split test data and make xgb.DMatrix
test_data  <- as.matrix(df [testIndexes,2:20])
test_label <-  as.numeric(as.factor(df[testIndexes,1]))-1#as.numeric(ytest)-1
test_matrix <- xgb.DMatrix(data = test_data, label = test_label)
numberOfClasses <- length(unique(ytrain))
xgb_params <- list("objective" = "multi:softmax",
                   "eval_metric" = "mlogloss",
                   "num_class" = numberOfClasses,
                   "subsample"=0.7)
bst_model <- xgb.train(params = xgb_params,
                       data = train_matrix,nrounds=100)

# Predict hold-out test set
test_pred <- predict(bst_model, newdata = test_matrix,type = "class")
xgt <- confusionMatrix(factor(test_pred),
                factor(test_label))
    cvxg[i] <- sum(diag(xgt$table))/sum(xgt$table)
    print(cvxg[i])
    
    ###Xgboost(10)
    bst_model <- xgb.train(params = xgb_params,
                       data = train_matrix,nrounds=10)

# Predict hold-out test set
test_pred10 <- predict(bst_model, newdata = test_matrix,type = "class")
xgt10 <- confusionMatrix(factor(test_pred10),
                factor(test_label))
    cvxg10[i] <- sum(diag(xgt10$table))/sum(xgt10$table)
    print(cvxg10[i])
}


cat("\nmean prediction accuracy of SBPMT:",mean(cvbpbtrf)*100) # CART
cat("\nmean prediction accuracy of RF:",mean(cvrf)*100)
cat("\nmean prediction accuracy of gradient boosting:",mean(cvgbm)*100)
cat("\nmean prediction accuracy of ADB:",mean(cvada)*100)
cat("\nmean prediction accuracy of xgboost(10):",mean(cvxg10)*100)
cat("\nmean prediction accuracy of xgboost(100):",mean(cvxg)*100)


cat("\nstd prediction accuracy of SBPMT:",sd(cvbpbtrf)*100) # CART
cat("\nstd prediction accuracy of RF:",sd(cvrf)*100)
cat("\nstd prediction accuracy of gradient boosting:",sd(cvgbm)*100)
cat("\nstd prediction accuracy of ADB:",sd(cvada)*100)
cat("\nstd prediction accuracy of Xgboost(10):",sd(cvxg10)*100)
cat("\nstd prediction accuracy of Xgboost(100):",sd(cvxg)*100)

```



#Waveform

```{r}

Waveform <- read.arff('dataset_60_waveform-5000.arff')




library(rpart)
set.seed(42)
df<-Waveform[sample(nrow(Waveform)),]


#   dmy<- dummyVars(" ~V2 + V3+ V7+V8", data = df)
#  cat_df <-  data.frame(predict(dmy, newdata = df))
#  num_df <- df[,c('V1','V4','V5','V6','V9','V10')]
# # 
# processed_df <- as.data.frame(cbind(cat_df,num_df))


library(RWeka)
#Create 10 equally size folds
folds <- caret::createFolds(factor(df$class), k = 10,list = FALSE)#cut(seq(1,nrow(df)),breaks=10,labels=FALSE)

cvpbt <- c()
cvbpbtrf <- c() 
cvdt <- c()
cvgbm <- c()
cvrf <- c()
clmt <- c()
cvada <- c()
cvxg <- c()
cvxg10 <- c()
for(i in 1:10){
    #Segement your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    xtest<- df[testIndexes, 1:40]
    #xtest<- xtest[,-2]
    ytest <- df[testIndexes,41]
    xtrain <- df[-testIndexes, 1:40]
    #xtrain <- xtrain[,-2]
    ytrain <- df[-testIndexes,41] 
    names(ytrain) <- row.names(xtrain)
    
     ###SBPMT
    
    sbpmt_pred <- SBPMT(n_tree=21,n_iteration=5,M=100,depth=6,xtrain=xtrain,ytrain=ytrain,xtest=xtest,step=0.5,size=20,alpha=0.7)
    rft <-table(sbpmt_pred,ytest)
    cvbpbtrf[i] <- sum(diag(rft))/sum(rft)
    print(cvbpbtrf[i])
    



# GradientBoost
lab_list = as.vector(sort(unique(ytrain)))
n_Class <- length(unique(ytrain))
boost_feat_test<- matrix(0,ncol=n_Class,nrow=nrow(xtest))  
        for (jClass in 1:n_Class) {
          #print('loop')
          yt = as.numeric(ytrain==lab_list[jClass]) # lablist[jClass]->1; rest->0
        gbm.model = gbm(yt~., data=xtrain, distribution = 'bernoulli',bag.fraction=0.7)

          boost_feat_test[,jClass] <- predict.gbm(gbm.model, xtest,type='response')
        }
        pred_y <- apply(boost_feat_test,1,function(row) lab_list[which.max(row)] )


    conf_mat= confusionMatrix(as.factor(pred_y),as.factor(ytest))
    acct_gbm <- (sum(diag(conf_mat$table)))/( sum(conf_mat$table))
    cvgbm[i] <-acct_gbm
    print(cvgbm[i])
    ### RandomForest 

     ytrain <- factor(ytrain)
    tree <- randomForest(ytrain~., data = xtrain)
    p <-predict(tree, xtest)  #ifelse(predict(tree, xtest)>0.5,1,0) #predict(tree, xtest,type = 'class') #ifelse(predict(tree, xtest,type = 'class')>0.5,1,0)
    confmatrix_treer <-table(p, ytest)
    acctr <- (sum(diag(confmatrix_treer)))/( sum(confmatrix_treer))
    cvrf[i] <-acctr
    print(cvrf[i])

    
    ###Adaboost
         data_adb <- df [-testIndexes,]
     data_adb$class<- as.factor(data_adb$class)
    #dt<- data.frame(ytrain=ytrain,xtrain=xtrain)
    ada.model = boosting(class~., data = data_adb)
    predada = predict.boosting(ada.model , newdata=df [testIndexes,])
    cvada[i] <- sum(diag(predada$confusion))/sum(predada$confusion)
    print(cvada[i])
    
    ###Xgboost(100)
    
            train_data   <- matrix(unlist(df[-testIndexes,1:40]), ncol = 40, byrow = FALSE)#as.matrix(as.data.frame(df[-testIndexes,1:41]))
    train_label  <- as.numeric(as.factor(df [-testIndexes,41]))-1
    train_matrix <- xgb.DMatrix(data = train_data, label = train_label)
    # split test data and make xgb.DMatrix
    test_data  <- matrix(unlist(df[testIndexes,1:40]), ncol = 40, byrow = FALSE)#df [testIndexes,1:41]
    test_label <-  as.numeric(as.factor(df [testIndexes,41]))-1#as.numeric(ytest)-1
    test_matrix <- xgb.DMatrix(data = test_data, label = test_label)
    numberOfClasses <- length(unique(ytrain))
    xgb_params <- list("objective" = "multi:softmax",
                       "eval_metric" = "mlogloss",
                       "num_class" = numberOfClasses,
                       "subsample"=0.7)
    bst_model <- xgb.train(params = xgb_params,
                           data = train_matrix,nrounds=100)
    
    # Predict hold-out test set
    test_pred <- predict(bst_model, newdata = test_matrix,type = "class")
    xgt <- confusionMatrix(factor(test_pred),
                    factor(test_label))
    cvxg[i] <- sum(diag(xgt$table))/sum(xgt$table)
    print(cvxg[i])
    
    ###Xgboost(10)
    bst_model <- xgb.train(params = xgb_params,
                       data = train_matrix,nrounds=10)

# Predict hold-out test set
test_pred10 <- predict(bst_model, newdata = test_matrix,type = "class")
xgt10 <- confusionMatrix(factor(test_pred10),
                factor(test_label))
    cvxg10[i] <- sum(diag(xgt10$table))/sum(xgt10$table)
    print(cvxg10[i])
}


cat("\nmean prediction accuracy of SBPMT:",mean(cvbpbtrf)*100) # CART
cat("\nmean prediction accuracy of RF:",mean(cvrf)*100)
cat("\nmean prediction accuracy of gradient boosting:",mean(cvgbm)*100)
cat("\nmean prediction accuracy of ADB:",mean(cvada)*100)
cat("\nmean prediction accuracy of xgboost(10):",mean(cvxg10)*100)
cat("\nmean prediction accuracy of xgboost(100):",mean(cvxg)*100)


cat("\nstd prediction accuracy of SBPMT:",sd(cvbpbtrf)*100) # CART
cat("\nstd prediction accuracy of RF:",sd(cvrf)*100)
cat("\nstd prediction accuracy of gradient boosting:",sd(cvgbm)*100)
cat("\nstd prediction accuracy of ADB:",sd(cvada)*100)
cat("\nstd prediction accuracy of Xgboost(10):",sd(cvxg10)*100)
cat("\nstd prediction accuracy of Xgboost(100):",sd(cvxg)*100)

```






#Pendigit

```{r}

Pendigitpar1 <- read.table('pendigits.tra',sep=',')
Pendigitpar2 <- read.table('pendigits.tes',sep=',')

Pendigits <- rbind(Pendigitpar1,Pendigitpar2)

library(rpart)
set.seed(42)
df<-Pendigits[sample(nrow(Pendigits)),]


#   dmy<- dummyVars(" ~V2 + V3+ V7+V8", data = df)
#  cat_df <-  data.frame(predict(dmy, newdata = df))
#  num_df <- df[,c('V1','V4','V5','V6','V9','V10')]
# # 
# processed_df <- as.data.frame(cbind(cat_df,num_df))


library(RWeka)
#Create 10 equally size folds
folds <- caret::createFolds(factor(df$V17), k = 10,list = FALSE)#cut(seq(1,nrow(df)),breaks=10,labels=FALSE)

cvpbt <- c()
cvbpbtrf <- c() 
cvdt <- c()
cvgbm <- c()
cvrf <- c()
clmt <- c()
cvada <- c()
cvxg <- c()
cvxg10 <- c()
for(i in 1:10){
    #Segement your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    xtest<- df[testIndexes, 1:16]
    #xtest<- xtest[,-2]
    ytest <- df[testIndexes,17]
    xtrain <- df[-testIndexes, 1:16]
    #xtrain <- xtrain[,-2]
    ytrain <- df[-testIndexes,17] 
    names(ytrain) <- row.names(xtrain)
    
     ###SBPMT
    
    sbpmt_pred <- SBPMT(n_tree=21,n_iteration=5,M=100,depth=6,xtrain=xtrain,ytrain=ytrain,xtest=xtest,step=0.5,size=20,alpha=0.7)
    rft <-table(sbpmt_pred,ytest)
    cvbpbtrf[i] <- sum(diag(rft))/sum(rft)
    print(cvbpbtrf[i])


# GradientBoost
  lab_list = as.vector(sort(unique(ytrain)))
n_Class <- length(unique(ytrain))
boost_feat_test<- matrix(0,ncol=n_Class,nrow=nrow(xtest))  
        for (jClass in 1:n_Class) {
          #print('loop')
          yt = as.numeric(ytrain==lab_list[jClass]) # lablist[jClass]->1; rest->0
        gbm.model = gbm(yt~., data=xtrain, distribution = 'bernoulli',bag.fraction=0.7)

          boost_feat_test[,jClass] <- predict.gbm(gbm.model, xtest,type='response')
        }
        pred_y <- apply(boost_feat_test,1,function(row) lab_list[which.max(row)] )


    conf_mat= confusionMatrix(as.factor(pred_y),as.factor(ytest))
    acct_gbm <- (sum(diag(conf_mat$table)))/( sum(conf_mat$table))
    cvgbm[i] <-acct_gbm
    print(cvgbm[i])
    ### RandomForest 

     ytrain <- factor(ytrain)
    tree <- randomForest(ytrain~., data = xtrain)
    p <-predict(tree, xtest)  #ifelse(predict(tree, xtest)>0.5,1,0) #predict(tree, xtest,type = 'class') #ifelse(predict(tree, xtest,type = 'class')>0.5,1,0)
    confmatrix_treer <-table(p, ytest)
    acctr <- (sum(diag(confmatrix_treer)))/( sum(confmatrix_treer))
    cvrf[i] <-acctr
    print(cvrf[i])

    
    ###Adaboost
         data_adb <- df [-testIndexes,]
     data_adb$V17<- as.factor(data_adb$V17)
    #dt<- data.frame(ytrain=ytrain,xtrain=xtrain)
    ada.model = boosting(V17~., data = data_adb)
    predada = predict.boosting(ada.model , newdata=df [testIndexes,])
    cvada[i] <- sum(diag(predada$confusion))/sum(predada$confusion)
    print(cvada[i])
    ###Xgboost(100)
            train_data   <- as.matrix(df[-testIndexes,1:16])
train_label  <- df [-testIndexes,17]#as.numeric(as.factor(df [-testIndexes,17]))-1
train_matrix <- xgb.DMatrix(data = train_data, label = train_label)
# split test data and make xgb.DMatrix
test_data  <-  as.matrix(df[testIndexes,1:16])#matrix(unlist(df[testIndexes,1:40]), ncol = 40, byrow = FALSE)#df [testIndexes,1:41]
test_label <-  df [testIndexes,17]#as.numeric(as.factor(df [testIndexes,41]))-1#as.numeric(ytest)-1
test_matrix <- xgb.DMatrix(data = test_data, label = test_label)
numberOfClasses <- length(unique(ytrain))
xgb_params <- list("objective" = "multi:softmax",
                   "eval_metric" = "mlogloss",
                   "num_class" = numberOfClasses,
                   "subsample"=0.7)
bst_model <- xgb.train(params = xgb_params,
                       data = train_matrix,nrounds=100)

# Predict hold-out test set
test_pred <- predict(bst_model, newdata = test_matrix,type = "class")
xgt <- confusionMatrix(factor(test_pred),
                factor(test_label))
    cvxg[i] <- sum(diag(xgt$table))/sum(xgt$table)
    print(cvxg[i])
    
    ###Xgboost(10)
    bst_model <- xgb.train(params = xgb_params,
                       data = train_matrix,nrounds=10)

# Predict hold-out test set
test_pred10 <- predict(bst_model, newdata = test_matrix,type = "class")
xgt10 <- confusionMatrix(factor(test_pred10),
                factor(test_label))
    cvxg10[i] <- sum(diag(xgt10$table))/sum(xgt10$table)
    print(cvxg10[i])
}


cat("\nmean prediction accuracy of SBPMT:",mean(cvbpbtrf)*100) # CART
cat("\nmean prediction accuracy of RF:",mean(cvrf)*100)
cat("\nmean prediction accuracy of gradient boosting:",mean(cvgbm)*100)
cat("\nmean prediction accuracy of ADB:",mean(cvada)*100)
cat("\nmean prediction accuracy of xgboost(10):",mean(cvxg10)*100)
cat("\nmean prediction accuracy of xgboost(100):",mean(cvxg)*100)


cat("\nstd prediction accuracy of SBPMT:",sd(cvbpbtrf)*100) # CART
cat("\nstd prediction accuracy of RF:",sd(cvrf)*100)
cat("\nstd prediction accuracy of gradient boosting:",sd(cvgbm)*100)
cat("\nstd prediction accuracy of ADB:",sd(cvada)*100)
cat("\nstd prediction accuracy of Xgboost(10):",sd(cvxg10)*100)
cat("\nstd prediction accuracy of Xgboost(100):",sd(cvxg)*100)

```



#Letter (may take a long time to run since we have 10 folds to run and the R code implementation hasn't been optimized)

```{r}

letter<- read.csv('letter-recognition.data',header=FALSE)
  #read.csv(url('https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data'),
                  # sep= ",", header=FALSE)

#write.csv(letter,'letter.csv')

library(rpart)
set.seed(42)
df<-letter[sample(nrow(letter)),]



library(RWeka)
#Create 10 equally size folds
folds <- caret::createFolds(df$V1, k = 10,list = FALSE)#cut(seq(1,nrow(df)),breaks=10,labels=FALSE)

cvpbt <- c()
cvbpbtrf <- c() 
cvdt <- c()
cvgbm <- c()
cvrf <- c()
clmt <- c()
cvada <- c()
cvxg <- c()
cvxg10 <- c()
for(i in 1:10){
    #Segement your data by fold using the which() function 
    testIndexes <- which(folds==i,arr.ind=TRUE)
    xtest<- df[testIndexes, 2:17]
    #xtest<- xtest[,-2]
    ytest <- df[testIndexes,1]
    xtrain <- df[-testIndexes, 2:17]
    #xtrain <- xtrain[,-2]
    ytrain <- df[-testIndexes,1] 
    names(ytrain) <- row.names(xtrain)
    
    
        ###SBPMT
    
    sbpmt_pred <- SBPMT(n_tree=21,n_iteration=5,M=100,depth=6,xtrain=xtrain,ytrain=ytrain,xtest=xtest,step=0.5,size=20,alpha=0.7)
    rft <-table(sbpmt_pred,ytest)
    cvbpbtrf[i] <- sum(diag(rft))/sum(rft)
    print(cvbpbtrf[i])
    

###GradientBoost
lab_list = as.vector(sort(unique(ytrain)))

n_Class <- length(unique(ytrain))
boost_feat_test<- matrix(0,ncol=n_Class,nrow=nrow(xtest))
        for (jClass in 1:n_Class) {
          #print('loop')
          yt = as.numeric(ytrain==lab_list[jClass]) # lablist[jClass]->1; rest->0
        gbm.model = gbm(yt~., data=xtrain, distribution = 'bernoulli',bag.fraction = 0.7)

          boost_feat_test[,jClass] <- predict.gbm(gbm.model, xtest,type='response')
        }
        pred_y <- apply(boost_feat_test,1,function(row) lab_list[which.max(row)] )

    conf_mat= confusionMatrix(as.factor(pred_y),as.factor(ytest))
    acct_gbm <- (sum(diag(conf_mat$table)))/( sum(conf_mat$table))
    cvgbm[i] <-acct_gbm
    print(cvgbm[i])
    ###RandomForest

     ytrain <- factor(ytrain)
    tree <- randomForest(ytrain~., data = xtrain)
    p <-predict(tree, xtest) 
    confmatrix_treer <-table(p, ytest)
    acctr <- (sum(diag(confmatrix_treer)))/( sum(confmatrix_treer))
    cvrf[i] <-acctr
    print(cvrf[i])


    ###Adaboost


        
n_Class <- length(unique(ytrain))
boost_feat_test<- matrix(0,ncol=n_Class,nrow=nrow(xtest))
        for (jClass in 1:n_Class) {
          #print('loop')
          yt = as.factor(as.numeric(ytrain==lab_list[jClass])) # lablist[jClass]->1; rest->0
        #gbm.model = gbm(yt~., data=xtrain, distribution = 'bernoulli')
ada.model = AdaBoostM1(yt~., data = xtrain,control=Weka_control(W = list(J48, M = 6)))#
          boost_feat_test[,jClass] <- predict(ada.model, xtest,type ='probability')[,2]
        }
        pred_y <- apply(boost_feat_test,1,function(row) lab_list[which.max(row)] )

    adb_tb <- confusionMatrix(as.factor(pred_y),as.factor(ytest))
    cvada[i] <- sum(diag(adb_tb$table))/sum(adb_tb$table)
    print(cvada[i])
# #     
### Xgboost(100)
    label <- as.numeric(as.factor(df [,1]))-1
            train_data   <- as.matrix(df [-testIndexes,2:17])
train_label  <- label[-testIndexes]#as.numeric(as.factor(df [-testIndexes,2]))-1
train_matrix <- xgb.DMatrix(data = train_data, label = train_label)
# split test data and make xgb.DMatrix
test_data  <- as.matrix(df [testIndexes,2:17])
test_label <-  label[testIndexes] #as.numeric(as.factor(df [testIndexes,2]))-1#as.numeric(ytest)-1
test_matrix <- xgb.DMatrix(data = test_data, label = test_label)
numberOfClasses <- length(unique(ytrain))
xgb_params <- list("objective" = "multi:softmax",
                   "eval_metric" = "mlogloss",
                   "num_class" = numberOfClasses,
                   "subsample"=0.7)
bst_model <- xgb.train(params = xgb_params,
                           data = train_matrix,nrounds=100)


test_pred <- predict(bst_model, newdata = test_matrix,type = "class")
xgt <- confusionMatrix(factor(test_pred),
                as.factor(test_label))
    cvxg[i] <- sum(diag(xgt$table))/sum(xgt$table)
    print(cvxg[i])

    ###Xgboost(10)
    bst_model <- xgb.train(params = xgb_params,
                       data = train_matrix,nrounds=10)

# Predict hold-out test set
test_pred10 <- predict(bst_model, newdata = test_matrix,type = "class")
xgt10 <- confusionMatrix(factor(test_pred10),
                factor(test_label))
    cvxg10[i] <- sum(diag(xgt10$table))/sum(xgt10$table)
    print(cvxg10[i])
}


cat("\nmean prediction accuracy of SBPMT:",mean(cvbpbtrf)*100) # CART
cat("\nmean prediction accuracy of RF:",mean(cvrf)*100)
cat("\nmean prediction accuracy of gradient boosting:",mean(cvgbm)*100)
cat("\nmean prediction accuracy of ADB:",mean(cvada)*100)
cat("\nmean prediction accuracy of xgboost(10):",mean(cvxg10)*100)
cat("\nmean prediction accuracy of xgboost(100):",mean(cvxg)*100)


cat("\nstd prediction accuracy of SBPMT:",sd(cvbpbtrf)*100) # CART
cat("\nstd prediction accuracy of RF:",sd(cvrf)*100)
cat("\nstd prediction accuracy of gradient boosting:",sd(cvgbm)*100)
cat("\nstd prediction accuracy of ADB:",sd(cvada)*100)
cat("\nstd prediction accuracy of Xgboost(10):",sd(cvxg10)*100)
cat("\nstd prediction accuracy of Xgboost(100):",sd(cvxg)*100)

```
